# pytest-patterns

A comprehensive guide to Python testing with pytest, covering everything from basic testing to advanced patterns and CI/CD integration.

## Overview

pytest is the de facto standard for testing Python applications. This skill provides comprehensive patterns, examples, and best practices for writing effective tests using pytest.

## What You'll Learn

- **Fixtures**: Modular test setup and teardown patterns
- **Parametrization**: Testing multiple inputs efficiently
- **Mocking**: Isolating code from external dependencies
- **Test Organization**: Structuring large test suites
- **Coverage**: Measuring and improving test coverage
- **CI/CD**: Integrating tests into continuous integration pipelines

## Quick Start

### Installation

```bash
# Basic installation
pip install pytest

# With coverage support
pip install pytest pytest-cov

# With async support
pip install pytest pytest-asyncio

# Full test environment
pip install pytest pytest-cov pytest-xdist pytest-mock pytest-timeout
```

### Your First Test

```python
# test_example.py
def test_simple_addition():
    assert 2 + 2 == 4

def test_string_operations():
    text = "hello world"
    assert text.upper() == "HELLO WORLD"
    assert "hello" in text
    assert len(text) == 11
```

Run it:

```bash
pytest test_example.py
```

## Core Features

### Simple and Powerful Syntax

pytest uses plain Python `assert` statements - no special assertion methods needed:

```python
# Clear and readable
assert result == expected_value
assert user.is_active
assert len(items) > 0
assert "error" not in response
```

### Automatic Test Discovery

pytest automatically finds your tests:

```
project/
├── src/
│   └── mypackage/
│       └── calculator.py
└── tests/
    ├── test_calculator.py      # ✓ Found
    ├── test_utils.py           # ✓ Found
    └── calculator_test.py      # ✓ Found
```

### Detailed Failure Messages

When tests fail, pytest shows you exactly what went wrong:

```python
def test_user_age():
    user = User(name="Alice", age=25)
    assert user.age == 30

# Output:
# AssertionError: assert 25 == 30
#  +  where 25 = User(name='Alice', age=25).age
```

## Fixtures - Reusable Test Setup

Fixtures are pytest's most powerful feature for test setup:

```python
import pytest

@pytest.fixture
def user():
    """Create a test user."""
    return User(name="Test User", email="test@example.com")

@pytest.fixture
def database():
    """Create and teardown test database."""
    db = Database()
    db.connect()
    yield db
    db.disconnect()

def test_user_creation(user, database):
    """Use both fixtures in a test."""
    database.save(user)
    assert database.count() == 1
```

### Fixture Scopes

Control how often fixtures are created:

```python
@pytest.fixture(scope="session")    # Once per test session
def database_engine():
    return create_engine("postgresql://test")

@pytest.fixture(scope="module")     # Once per test module
def api_client():
    return APIClient()

@pytest.fixture(scope="function")   # Once per test (default)
def temp_file():
    return create_temp_file()
```

## Parametrization - Test Multiple Cases

Run the same test with different inputs:

```python
import pytest

@pytest.mark.parametrize("input_value,expected", [
    ("hello", "HELLO"),
    ("world", "WORLD"),
    ("python", "PYTHON"),
])
def test_uppercase(input_value, expected):
    assert input_value.upper() == expected

# Runs 3 tests:
# test_uppercase[hello-HELLO]
# test_uppercase[world-WORLD]
# test_uppercase[python-PYTHON]
```

### Multiple Parameters

```python
@pytest.mark.parametrize("x", [1, 2])
@pytest.mark.parametrize("y", [10, 20])
def test_addition(x, y):
    assert x + y > 0

# Runs 4 tests: (1,10), (1,20), (2,10), (2,20)
```

## Mocking - Isolate Your Tests

Use monkeypatch for safe, automatic cleanup:

```python
def test_environment_variable(monkeypatch):
    monkeypatch.setenv("API_KEY", "test-key")
    assert os.getenv("API_KEY") == "test-key"
    # Automatically restored after test

def test_api_call(monkeypatch):
    def mock_get(*args, **kwargs):
        return MockResponse({"status": "ok"})

    monkeypatch.setattr(requests, "get", mock_get)
    result = fetch_data()
    assert result["status"] == "ok"
```

## Test Organization

### Recommended Structure

```
project/
├── src/
│   └── mypackage/
│       ├── __init__.py
│       ├── models.py
│       └── services.py
├── tests/
│   ├── __init__.py
│   ├── conftest.py           # Shared fixtures
│   ├── unit/
│   │   ├── __init__.py
│   │   ├── test_models.py
│   │   └── test_services.py
│   ├── integration/
│   │   └── test_api.py
│   └── e2e/
│       └── test_workflows.py
├── pytest.ini                # Configuration
└── requirements-dev.txt
```

### Using conftest.py

Share fixtures across tests:

```python
# tests/conftest.py
import pytest

@pytest.fixture(scope="session")
def database():
    """Available to all tests."""
    db = Database()
    db.connect()
    yield db
    db.disconnect()

@pytest.fixture
def clean_database(database):
    """Reset database before each test."""
    database.clear()
    return database
```

## Markers - Organize and Filter Tests

Mark tests for selective execution:

```python
import pytest

@pytest.mark.slow
def test_long_running_operation():
    time.sleep(5)
    assert True

@pytest.mark.integration
def test_external_api():
    response = requests.get("https://api.example.com")
    assert response.status_code == 200

@pytest.mark.skip(reason="Feature not implemented")
def test_future_feature():
    pass

@pytest.mark.xfail(reason="Known bug")
def test_with_known_issue():
    assert buggy_function() == expected_value
```

Run specific tests:

```bash
pytest -m slow                    # Run only slow tests
pytest -m "not slow"              # Skip slow tests
pytest -m "integration and not slow"  # Complex filtering
```

## Coverage - Measure Test Effectiveness

### Generate Coverage Reports

```bash
# Terminal report
pytest --cov=mypackage tests/

# HTML report
pytest --cov=mypackage --cov-report=html tests/

# Fail if coverage below threshold
pytest --cov=mypackage --cov-fail-under=80 tests/
```

### Coverage Output

```
---------- coverage: platform darwin, python 3.11.0 -----------
Name                      Stmts   Miss  Cover
---------------------------------------------
mypackage/__init__.py         4      0   100%
mypackage/models.py          42      2    95%
mypackage/services.py        38      5    87%
mypackage/utils.py           15      1    93%
---------------------------------------------
TOTAL                        99      8    92%
```

### Configuration

```ini
# pytest.ini
[pytest]
addopts =
    --cov=mypackage
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=80
```

## Common Testing Patterns

### Testing Exceptions

```python
import pytest

def test_divide_by_zero():
    with pytest.raises(ZeroDivisionError):
        result = 10 / 0

def test_validation_error():
    with pytest.raises(ValueError, match="Invalid email"):
        validate_email("not-an-email")
```

### Testing Logs

```python
def test_logging(caplog):
    import logging
    logger = logging.getLogger("myapp")

    logger.info("Processing started")
    logger.warning("High memory usage")

    assert "Processing started" in caplog.text
    assert any(record.levelname == "WARNING" for record in caplog.records)
```

### Testing Output

```python
def test_print_output(capsys):
    print("Hello, World!")
    captured = capsys.readouterr()
    assert "Hello, World!" in captured.out
```

### Testing with Temporary Files

```python
def test_file_processing(tmp_path):
    # tmp_path is a pytest fixture
    test_file = tmp_path / "data.txt"
    test_file.write_text("test content")

    result = process_file(test_file)
    assert result.success
```

## Running Tests

### Basic Commands

```bash
# Run all tests
pytest

# Run specific file
pytest tests/test_models.py

# Run specific test
pytest tests/test_models.py::test_user_creation

# Run tests matching pattern
pytest -k "user and not delete"

# Stop on first failure
pytest -x

# Show local variables on failure
pytest -l

# Verbose output
pytest -v

# Quiet output
pytest -q
```

### Advanced Options

```bash
# Run in parallel (requires pytest-xdist)
pytest -n auto

# Rerun failed tests
pytest --lf

# Run tests that failed in last run
pytest --ff

# Show slowest tests
pytest --durations=10

# Drop into debugger on failure
pytest --pdb

# Show print statements
pytest -s
```

## CI/CD Integration

### GitHub Actions

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10', '3.11']

    steps:
    - uses: actions/checkout@v3
    - uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - run: pip install -e .[dev]
    - run: pytest --cov --cov-report=xml
    - uses: codecov/codecov-action@v3
```

### GitLab CI

```yaml
test:
  image: python:3.11
  script:
    - pip install -e .[dev]
    - pytest --cov --cov-report=xml
  coverage: '/(?i)total.*? (100(?:\.0+)?\%|[1-9]?\d(?:\.\d+)?\%)$/'
```

## Configuration

### pytest.ini

```ini
[pytest]
# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Output
addopts =
    -ra
    --strict-markers
    --strict-config
    --showlocals

# Markers
markers =
    slow: marks tests as slow
    integration: marks tests as integration tests
    unit: marks tests as unit tests
```

### pyproject.toml

```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
addopts = ["-ra", "--strict-markers", "--cov=mypackage"]
markers = [
    "slow: marks tests as slow",
    "integration: marks tests as integration tests",
]
```

## Useful Plugins

Essential pytest plugins:

- **pytest-cov**: Coverage reporting
- **pytest-xdist**: Parallel test execution
- **pytest-asyncio**: Async/await support
- **pytest-mock**: Enhanced mocking
- **pytest-timeout**: Test timeouts
- **pytest-randomly**: Randomize test order
- **pytest-html**: HTML test reports

Install them:

```bash
pip install pytest-cov pytest-xdist pytest-asyncio pytest-mock
```

## Best Practices

### 1. Write Isolated Tests

Each test should be independent and not rely on other tests:

```python
# Bad - tests depend on order
def test_create_user():
    global user
    user = create_user("Alice")

def test_update_user():
    user.update(age=30)  # Depends on previous test

# Good - each test is independent
def test_create_user():
    user = create_user("Alice")
    assert user.name == "Alice"

def test_update_user():
    user = create_user("Bob")
    user.update(age=30)
    assert user.age == 30
```

### 2. Use Descriptive Names

Make test names explain what they test:

```python
# Bad
def test_user():
    pass

# Good
def test_user_creation_with_valid_email():
    pass

def test_user_login_fails_with_wrong_password():
    pass
```

### 3. Follow AAA Pattern

Arrange, Act, Assert:

```python
def test_shopping_cart_total():
    # Arrange
    cart = ShoppingCart()
    cart.add_item(Product(name="Book", price=10.99))
    cart.add_item(Product(name="Pen", price=2.50))

    # Act
    total = cart.get_total()

    # Assert
    assert total == 13.49
```

### 4. Don't Test Implementation Details

Test behavior, not implementation:

```python
# Bad - testing internal implementation
def test_user_password_hash():
    user = User(password="secret")
    assert user._password_hash.startswith("$2b$")

# Good - testing behavior
def test_user_can_login_with_correct_password():
    user = User(password="secret")
    assert user.verify_password("secret") is True
    assert user.verify_password("wrong") is False
```

### 5. Keep Tests Fast

- Use in-memory databases for tests
- Mock external services
- Use appropriate fixture scopes
- Run slow tests separately with markers

## Troubleshooting

### Tests Not Found

```bash
# See what pytest discovers
pytest --collect-only

# Ensure file naming is correct
test_*.py or *_test.py

# Check function naming
test_*
```

### Import Errors

```bash
# Install package in development mode
pip install -e .

# Or add to PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"
```

### Fixture Not Found

```bash
# List available fixtures
pytest --fixtures

# Check fixture is in conftest.py or same file
# Verify scope is appropriate
```

## Learning Path

1. **Beginner**: Basic tests, simple fixtures, parametrization
2. **Intermediate**: Complex fixtures, mocking, test organization
3. **Advanced**: Custom plugins, advanced parametrization, property-based testing
4. **Expert**: Performance optimization, distributed testing, custom reporters

## Resources

### Official Documentation

- pytest docs: https://docs.pytest.org/
- pytest GitHub: https://github.com/pytest-dev/pytest
- Plugin list: https://docs.pytest.org/en/latest/reference/plugin_list.html

### Tutorials

- Real Python pytest guide: https://realpython.com/pytest-python-testing/
- pytest with Eric: https://testandcode.com/
- Full pytest documentation: https://docs.pytest.org/en/stable/contents.html

### Books

- Python Testing with pytest by Brian Okken
- Test-Driven Development with Python by Harry Percival

### Community

- pytest Discord: https://discord.com/invite/pytest-dev
- Stack Overflow: https://stackoverflow.com/questions/tagged/pytest

## Quick Reference

### Common Commands

```bash
pytest                          # Run all tests
pytest test_file.py            # Run specific file
pytest -k "pattern"            # Run tests matching pattern
pytest -m marker               # Run tests with marker
pytest -x                      # Stop on first failure
pytest --lf                    # Rerun last failed
pytest -v                      # Verbose
pytest -q                      # Quiet
pytest --cov=pkg              # Coverage report
pytest -n auto                # Parallel execution
```

### Common Fixtures

```python
tmp_path        # Temporary directory (pathlib.Path)
tmp_path_factory # Factory for temporary directories
capsys          # Capture stdout/stderr
caplog          # Capture log messages
monkeypatch     # Modify objects/environment
request         # Request object for fixtures
```

### Common Markers

```python
@pytest.mark.parametrize    # Run with multiple inputs
@pytest.mark.skip           # Skip test
@pytest.mark.skipif         # Conditional skip
@pytest.mark.xfail          # Expected to fail
@pytest.mark.slow           # Custom marker
```

---

**Version**: 1.0.0
**Last Updated**: October 2025
**License**: MIT
